# learn-to-build-a-llm

从零开始构建一个大语言模型

## 1.预训练一个简单的GPT

### pretrain

| dataset/input.txt        | 莎士比亚文本                                                                        |
|--------------------------|-------------------------------------------------------------------------------|
| bigram.py                | 实现一个最简单的GPT                                                                   |
| bigram_self_attention.py | 构建Transformer，加入self-attention多头自注意、残差链接、Dropout、前馈层FeedForward、层规范layer norm |
| 最简单的模型实现.ipynb           | 如何一步一步实现的notebook文件                                                           |
